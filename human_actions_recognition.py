# -*- coding: utf-8 -*-
"""human actions recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xoj5Ej8YFzfZ6Uxa1wt1r8aaXoWp9CtO
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
# %matplotlib inline

from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)

# Create a Matplotlib figure with the specific size
plt.figure(figsize=(20, 20))

# read a dataset
drive.mount('/content/drive')
dataset = '/content/drive/MyDrive/dataset/UCF50'
all_classes = os.listdir(dataset)

# Generate a list of 20 random class indices
random_indices = random.sample(range(len(all_classes)), 20)

# Iterate through the randomly selected class indices
for i, random_index in enumerate(random_indices, 1):

    class_name = all_classes[random_index]
    video_files = os.listdir(f'/content/drive/My Drive/dataset/UCF50/{class_name}')
    selected_video_file = random.choice(video_files)

    video_reader = cv2.VideoCapture(f'/content/drive/My Drive/dataset/UCF50/{class_name}/{selected_video_file}')
    _, bgr_frame = video_reader.read()
    video_reader.release()
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame
    cv2.putText(rgb_frame, class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

    # Display the frame.
    plt.subplot(5, 4, i)
    plt.imshow(rgb_frame)
    plt.axis('off')



image_height, image_width = 64, 64

# Specify the number of frames per sequence fed to the model
sequence_length = 20
dataset_dir = "/content/drive/My Drive/dataset/UCF50"

# Specify the list of classes(actions) used for training
classes_list = all_classes
#classes_list = ["WalkingWithDog", "TaiChi", "Swing", "HorseRace"]

def extract_frames(video_path):

    # working on the frames of videos
    frames_list = []
    video_reader = cv2.VideoCapture(video_path)
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    skip_frames_window = max(int(video_frames_count / sequence_length), 1)

    for frame_counter in range(sequence_length):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
        successful, frame = video_reader.read()
        if not successful:
            break

        # Resize the frame to the specified height and width.
        resized_frame = cv2.resize(frame, (image_height, image_width))

        # Normalize the resized frame
        normalized_frame = resized_frame / 255
        frames_list.append(normalized_frame)

    video_reader.release()
    return frames_list

def extract_data():

    # Lists to store features, labels, and video file paths
    data = []
    labels = []
    video_paths = []

    for class_index, class_name in enumerate(classes_list):
        print(f'Extracting Data of Class: {class_name}')
        files_list = os.listdir(os.path.join(dataset_dir, class_name))

        for file_name in files_list:
            video_path = os.path.join(dataset_dir, class_name, file_name)
            frames = extract_frames(video_path)

            # Check if the extracted frames match the specified sequence length
            if len(frames) == sequence_length:
                data.append(frames)
                labels.append(class_index)
                video_paths.append(video_path)

    # Convert lists to numpy arrays
    data = np.asarray(data)
    labels = np.array(labels)

    # Save the data and labels into an NPZ file
    save_path = "/content/extracted_data.npz"
    np.savez(save_path, data=data, labels=labels, video_paths=video_paths)
    return data, labels, video_paths



import cupy as cp  # Import CuPy for GPU acceleration

def extract_frames(video_path):
    # working on the frames of videos
    frames_list = []
    video_reader = cv2.VideoCapture(video_path)
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    skip_frames_window = max(int(video_frames_count / sequence_length), 1)

    for frame_counter in range(sequence_length):
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
        successful, frame = video_reader.read()
        if not successful:
            break

        # Resize the frame to the specified height and width using CuPy for GPU acceleration
        frame = cp.asarray(frame)  # Convert frame to CuPy array
        resized_frame = cp.asnumpy(cp.resize(frame, (image_height, image_width)))  # Resize using CuPy
        normalized_frame = resized_frame / 255
        frames_list.append(normalized_frame)

    video_reader.release()
    return frames_list

def extract_data():
    # Lists to store features, labels, and video file paths
    data = []
    labels = []
    video_paths = []

    for class_index, class_name in enumerate(classes_list):
        print(f'Extracting Data of Class: {class_name}')
        files_list = os.listdir(os.path.join(dataset_dir, class_name))

        for file_name in files_list:
            video_path = os.path.join(dataset_dir, class_name, file_name)
            frames = extract_frames(video_path)

            # Check if the extracted frames match the specified sequence length
            if len(frames) == sequence_length:
                data.append(frames)
                labels.append(class_index)
                video_paths.append(video_path)

    # Convert lists to numpy arrays
    data = np.asarray(data)
    labels = np.array(labels)

    # Save the data and labels into an NPZ file
    save_path = "/content/extracted_data.npz"
    np.savez(save_path, data=data, labels=labels, video_paths=video_paths)
    return data, labels, video_paths

data, class_indexes, video_paths = extract_data()

# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(class_indexes)

# Split the Data into Training and Testing Set
features_train, features_test, labels_train, labels_test = train_test_split(data, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)

def construct_convlstm_model():

    model = Sequential()

    # The Model Architecture
    model.add(ConvLSTM2D(filters=4, kernel_size=(3, 3), activation='tanh', data_format="channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape=(sequence_length,
                                                                                      image_height, image_width, 3)))

    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))

    model.add(ConvLSTM2D(filters=8, kernel_size=(3, 3), activation='tanh', data_format="channels_last",
                         recurrent_dropout=0.2, return_sequences=True))

    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))

    model.add(ConvLSTM2D(filters=14, kernel_size=(3, 3), activation='tanh', data_format="channels_last",
                         recurrent_dropout=0.2, return_sequences=True))

    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))

    model.add(ConvLSTM2D(filters=16, kernel_size=(3, 3), activation='tanh', data_format="channels_last",
                         recurrent_dropout=0.2, return_sequences=True))

    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))

    model.add(Flatten())

    model.add(Dense(len(classes_list), activation="softmax"))

    model.summary()
    return model

convlstm_model = construct_convlstm_model()
print("Model Created Successfully!")

plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)

# Create an instance of Early Stopping Callback to prevent overfitting
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)

# Compile the model and specify loss function, optimizer, and metrics
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=["accuracy"])

convlstm_model_training_history = convlstm_model.fit(x=features_train, y=labels_train, epochs=50, batch_size=4,
                                                     shuffle=True, validation_split=0.2,
                                                     callbacks=[early_stopping_callback])

# Evaluate the trained model on the test data
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)

evaluation_loss, evaluation_accuracy = model_evaluation_history

# Save the trained model.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{evaluation_loss}___Accuracy_{evaluation_accuracy}.h5'
convlstm_model.save(model_file_name)

def plot_metrics(training_history, metric_1_name, metric_2_name, plot_title):

    # Get metric values using metric names as identifiers.
    metric_1_values = training_history.history[metric_1_name]
    metric_2_values = training_history.history[metric_2_name]

    # Create range of epochs for x-axis.
    epochs = range(len(metric_1_values))

    plt.plot(epochs, metric_1_values, 'blue', label=metric_1_name)
    plt.plot(epochs, metric_2_values, 'red', label=metric_2_name)
    plt.title(str(plot_title))
    plt.legend()

# Visualization of the training and validation loss metrics
plot_metrics(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

# Visualization of the training and validation accuracy metrices
plot_metrics(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

def construct_lrcn_model():

    model = Sequential()

    # The Model Architecture
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'),
                              input_shape=(sequence_length, image_height, image_width, 3)))

    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))

    model.add(TimeDistributed(Flatten()))

    model.add(LSTM(32))

    model.add(Dense(len(classes_list), activation='softmax'))

    model.summary()
    return model

lrcn_model = construct_lrcn_model()
print("Model Created Successfully!")

plot_model(lrcn_model, to_file='LRCN_model_structure_plot.png', show_shapes=True, show_layer_names=True)

# Create an instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True)

# Compile the model and specify loss function, optimizer, and metrics
lrcn_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=["accuracy"])

lrcn_model_training_history = lrcn_model.fit(x=features_train, y=labels_train, epochs=70, batch_size=4,
                                             shuffle=True, validation_split=0.2, callbacks=[early_stopping_callback])

# Evaluate the trained model on the test data
model_evaluation_history = lrcn_model.evaluate(features_test, labels_test)

# Get the loss and accuracy from model_evaluation_history
evaluation_loss, evaluation_accuracy = model_evaluation_history

# Save the trained model.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{evaluation_loss}___Accuracy_{evaluation_accuracy}.h5'
lrcn_model.save(model_file_name)

# Visualization of the training and validation loss metrics
plot_metrics(lrcn_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

# Visualization of the training and validation accuracy metrics
plot_metrics(lrcn_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

!pip install pytube

from pytube import YouTube as YT

def download_youtube_videos(url, output_dir):
    try:

        video = YT(url)
        vid_title = video.title
        # Get the highest resolution video stream
        vid_stream = video.streams.get_highest_resolution()
        out_file_path = f'{output_dir}/{vid_title}.mp4'
        vid_stream.download(output_dir)
        return vid_title

    except Exception as error:
        print(f"An error occurred: {error}")

# Download a Video from youtube
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok = True)
video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE&ab_channel=ChipmunkStyle', test_videos_directory)
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'

# Define a function to perform action recognition on a video and overlay the predicted action on the frames
def predict_on_video(video_file_path, output_file_path, sequence_length):

    video_reader = cv2.VideoCapture(video_file_path)
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    frames_queue = deque(maxlen=sequence_length)

    # Initialize a variable to store the predicted action
    predicted_action = ''

    while video_reader.isOpened():
        successful, frame = video_reader.read()
        if not successful:
            break

        resized_frame = cv2.resize(frame, (image_height, image_width))
        normalized_frame = resized_frame / 255
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue equals the sequence length
        if len(frames_queue) == sequence_length:
            predicted_labels_probabilities = lrcn_model.predict(np.expand_dims(frames_queue, axis=0))[0]
            predicted_label = np.argmax(predicted_labels_probabilities)
            predicted_action = classes_list[predicted_label]

        # Write the frame with overlayed action to the output video
        cv2.putText(frame, predicted_action, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        video_writer.write(frame)

    video_reader.release()
    video_writer.release()

# Displaying the output video
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{sequence_length}.mp4'
predict_on_video(input_video_file_path, output_video_file_path, sequence_length)
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()

def predict_single_action(video_file_path1, sequence_length):

    video_reader = cv2.VideoCapture(video_file_path1)
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    frames_list = []

    predicted_class_name = ''
    confidence = 0.0

    # Get the number of frames in the video
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the interval after which frames will be added to the list
    skip_frames_window = max(int(video_frames_count / sequence_length), 1)

    for frame_counter in range(sequence_length):

        # Set the current frame position of the video
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
        successful, frame = video_reader.read()

        if not successful:
            break

        resized_frame = cv2.resize(frame, (image_height, image_width))
        normalized_frame = resized_frame / 255
        frames_list.append(normalized_frame)

    predicted_labels_probabilities = lrcn_model.predict(np.expand_dims(frames_list, axis=0))[0]
    predicted_label = np.argmax(predicted_labels_probabilities)
    predicted_class_name = classes_list[predicted_label]
    confidence = predicted_labels_probabilities[predicted_label]

    video_reader.release()
    return predicted_class_name, confidence

# Download a Video from youtube
video_title1 = download_youtube_videos('https://www.youtube.com/watch?v=6U8ipxSRAUA&ab_channel=AtTheRaces', test_videos_directory)
#https://www.youtube.com/watch?v=6U8ipxSRAUA&ab_channel=AtTheRaces

input_video_file_path1 = f'{test_videos_directory}/{video_title1}.mp4'
predicted_action, confidence = predict_single_action(input_video_file_path1, sequence_length)

# Displaying the video
video_clip = VideoFileClip(input_video_file_path1, audio=False, target_resolution=(300, None))
video_clip.ipython_display(maxduration=300)  # Set maxduration

print("Predicted Action:", predicted_action)
print("Confidence:", confidence)

